{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Essay Scoring\n",
    "\n",
    "## Data\n",
    "Description: https://www.kaggle.com/c/asap-aes/data\n",
    "\n",
    "The dataset contains essay and scores they received. There are three raters for two domains, but as we'll see, we will focus on only one rater. Moreover, there are \"rater trait\" variables which we can also discard, because we won't have such variables available on test sets.\n",
    "\n",
    "The essay text had _entities_ removed and replaced with there generic name using an off-the-shelf [named entity recognition model](https://nlp.stanford.edu/software/CRF-NER.shtml). This includes seven entities: \"PERSON\", \"ORGANIZATION\", \"LOCATION\", \"DATE\", \"TIME\", \"MONEY\", and \"PERCENT\". For example, \"Mike Smith ate dinner\" --> \"PERSON1 ate dinner.\" Note that the word was numbered too.\n",
    "\n",
    "Additionally, six other entities were replaced:\n",
    "1. \"EMAIL\" (anything that looks like an e-mail address)\n",
    "2. \"NUM\" (word containing digits or non-alphanumeric symbols)\n",
    "3. \"CAPS\" (any capitalized word that doesn't begin a sentence, except in essays where more than 20% of the characters are capitalized letters)\n",
    "4. \"DR\" (any word following \"Dr.\" with or without the period, with any capitalization, that doesn't fall into any of the above)\n",
    "5. \"CITY\" (any city)\n",
    "6. \"STATE\" (any state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12978, 28)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training set\n",
    "train = pd.read_excel('asap-aes/training_set_rel3.xlsx')\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>rater3_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>rater1_domain2</th>\n",
       "      <th>rater2_domain2</th>\n",
       "      <th>domain2_score</th>\n",
       "      <th>...</th>\n",
       "      <th>rater2_trait3</th>\n",
       "      <th>rater2_trait4</th>\n",
       "      <th>rater2_trait5</th>\n",
       "      <th>rater2_trait6</th>\n",
       "      <th>rater3_trait1</th>\n",
       "      <th>rater3_trait2</th>\n",
       "      <th>rater3_trait3</th>\n",
       "      <th>rater3_trait4</th>\n",
       "      <th>rater3_trait5</th>\n",
       "      <th>rater3_trait6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "\n",
       "   rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n",
       "0             4.0             4.0             NaN            8.0   \n",
       "1             5.0             4.0             NaN            9.0   \n",
       "2             4.0             3.0             NaN            7.0   \n",
       "3             5.0             5.0             NaN           10.0   \n",
       "4             4.0             4.0             NaN            8.0   \n",
       "\n",
       "   rater1_domain2  rater2_domain2  domain2_score  ...  rater2_trait3  \\\n",
       "0             NaN             NaN            NaN  ...            NaN   \n",
       "1             NaN             NaN            NaN  ...            NaN   \n",
       "2             NaN             NaN            NaN  ...            NaN   \n",
       "3             NaN             NaN            NaN  ...            NaN   \n",
       "4             NaN             NaN            NaN  ...            NaN   \n",
       "\n",
       "   rater2_trait4  rater2_trait5  rater2_trait6  rater3_trait1  rater3_trait2  \\\n",
       "0            NaN            NaN            NaN            NaN            NaN   \n",
       "1            NaN            NaN            NaN            NaN            NaN   \n",
       "2            NaN            NaN            NaN            NaN            NaN   \n",
       "3            NaN            NaN            NaN            NaN            NaN   \n",
       "4            NaN            NaN            NaN            NaN            NaN   \n",
       "\n",
       "   rater3_trait3  rater3_trait4  rater3_trait5  rater3_trait6  \n",
       "0            NaN            NaN            NaN            NaN  \n",
       "1            NaN            NaN            NaN            NaN  \n",
       "2            NaN            NaN            NaN            NaN  \n",
       "3            NaN            NaN            NaN            NaN  \n",
       "4            NaN            NaN            NaN            NaN  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View head\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "The total dataset has 28 columns but many of them turn out to be useless for our purposes, so we will drop them. \n",
    "There are 8 essay prompts. Let's group by `essay_prompt` and count the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>rater3_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>rater1_domain2</th>\n",
       "      <th>rater2_domain2</th>\n",
       "      <th>domain2_score</th>\n",
       "      <th>rater1_trait1</th>\n",
       "      <th>...</th>\n",
       "      <th>rater2_trait3</th>\n",
       "      <th>rater2_trait4</th>\n",
       "      <th>rater2_trait5</th>\n",
       "      <th>rater2_trait6</th>\n",
       "      <th>rater3_trait1</th>\n",
       "      <th>rater3_trait2</th>\n",
       "      <th>rater3_trait3</th>\n",
       "      <th>rater3_trait4</th>\n",
       "      <th>rater3_trait5</th>\n",
       "      <th>rater3_trait6</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>essay_set</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1783</td>\n",
       "      <td>1783</td>\n",
       "      <td>1783</td>\n",
       "      <td>1783</td>\n",
       "      <td>0</td>\n",
       "      <td>1783</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1800</td>\n",
       "      <td>1800</td>\n",
       "      <td>1800</td>\n",
       "      <td>1800</td>\n",
       "      <td>0</td>\n",
       "      <td>1800</td>\n",
       "      <td>1800</td>\n",
       "      <td>1800</td>\n",
       "      <td>1800</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1726</td>\n",
       "      <td>1726</td>\n",
       "      <td>1726</td>\n",
       "      <td>1726</td>\n",
       "      <td>0</td>\n",
       "      <td>1726</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1772</td>\n",
       "      <td>1772</td>\n",
       "      <td>1771</td>\n",
       "      <td>1771</td>\n",
       "      <td>0</td>\n",
       "      <td>1771</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1805</td>\n",
       "      <td>1805</td>\n",
       "      <td>1805</td>\n",
       "      <td>1805</td>\n",
       "      <td>0</td>\n",
       "      <td>1805</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1800</td>\n",
       "      <td>1800</td>\n",
       "      <td>1800</td>\n",
       "      <td>1800</td>\n",
       "      <td>0</td>\n",
       "      <td>1800</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1569</td>\n",
       "      <td>1569</td>\n",
       "      <td>1569</td>\n",
       "      <td>1569</td>\n",
       "      <td>0</td>\n",
       "      <td>1569</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1569</td>\n",
       "      <td>...</td>\n",
       "      <td>1569</td>\n",
       "      <td>1569</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>723</td>\n",
       "      <td>723</td>\n",
       "      <td>723</td>\n",
       "      <td>723</td>\n",
       "      <td>128</td>\n",
       "      <td>723</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>723</td>\n",
       "      <td>...</td>\n",
       "      <td>723</td>\n",
       "      <td>723</td>\n",
       "      <td>723</td>\n",
       "      <td>723</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows √ó 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           essay_id  essay  rater1_domain1  rater2_domain1  rater3_domain1  \\\n",
       "essay_set                                                                    \n",
       "1              1783   1783            1783            1783               0   \n",
       "2              1800   1800            1800            1800               0   \n",
       "3              1726   1726            1726            1726               0   \n",
       "4              1772   1772            1771            1771               0   \n",
       "5              1805   1805            1805            1805               0   \n",
       "6              1800   1800            1800            1800               0   \n",
       "7              1569   1569            1569            1569               0   \n",
       "8               723    723             723             723             128   \n",
       "\n",
       "           domain1_score  rater1_domain2  rater2_domain2  domain2_score  \\\n",
       "essay_set                                                                 \n",
       "1                   1783               0               0              0   \n",
       "2                   1800            1800            1800           1800   \n",
       "3                   1726               0               0              0   \n",
       "4                   1771               0               0              0   \n",
       "5                   1805               0               0              0   \n",
       "6                   1800               0               0              0   \n",
       "7                   1569               0               0              0   \n",
       "8                    723               0               0              0   \n",
       "\n",
       "           rater1_trait1  ...  rater2_trait3  rater2_trait4  rater2_trait5  \\\n",
       "essay_set                 ...                                                \n",
       "1                      0  ...              0              0              0   \n",
       "2                      0  ...              0              0              0   \n",
       "3                      0  ...              0              0              0   \n",
       "4                      0  ...              0              0              0   \n",
       "5                      0  ...              0              0              0   \n",
       "6                      0  ...              0              0              0   \n",
       "7                   1569  ...           1569           1569              0   \n",
       "8                    723  ...            723            723            723   \n",
       "\n",
       "           rater2_trait6  rater3_trait1  rater3_trait2  rater3_trait3  \\\n",
       "essay_set                                                               \n",
       "1                      0              0              0              0   \n",
       "2                      0              0              0              0   \n",
       "3                      0              0              0              0   \n",
       "4                      0              0              0              0   \n",
       "5                      0              0              0              0   \n",
       "6                      0              0              0              0   \n",
       "7                      0              0              0              0   \n",
       "8                    723            128            128            128   \n",
       "\n",
       "           rater3_trait4  rater3_trait5  rater3_trait6  \n",
       "essay_set                                               \n",
       "1                      0              0              0  \n",
       "2                      0              0              0  \n",
       "3                      0              0              0  \n",
       "4                      0              0              0  \n",
       "5                      0              0              0  \n",
       "6                      0              0              0  \n",
       "7                      0              0              0  \n",
       "8                    128            128            128  \n",
       "\n",
       "[8 rows x 27 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# essay_set\n",
    "train.groupby('essay_set').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that:\n",
    "1. There are three scorers for domain1, however, the _resolved score_, `domain1_score`, is our target variable. You can tell `domain1_score` is the target variable by seeing that its range is consistent with the sample submissions in `valid_sample_submission_5_column.csv`. Since the three scores are superfluous, and we won't have those variables in we drop them (`rater1_domain1`, `rater2_domain1`, `rater3_domain1`).\n",
    "2. Each `essay_set` has its own prompt. The prompts can vary widely, so I focus on just one to start. I will look at only `essay_set==1`. Then, if the model is working, then I will generalize to all eight essay sets.\n",
    "3. The _domain2_ scores apply to only `essay_id==2`, so I will drop them for now.\n",
    "4. There are 6 trains for the three raters. I think these are characteristics of the person who is rating the essay. These personal characterestics should not be included because we cannot use them as predictive variables on test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplify data\n",
    "train1 = train.loc[train.essay_set==1,:].drop(columns=['rater1_domain1', 'rater2_domain1', 'rater3_domain1']).iloc[:,0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1783.000000\n",
       "mean        8.528323\n",
       "std         1.538565\n",
       "min         2.000000\n",
       "25%         8.000000\n",
       "50%         8.000000\n",
       "75%        10.000000\n",
       "max        12.000000\n",
       "Name: domain1_score, dtype: float64"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# descriptive statistics\n",
    "train1.domain1_score.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable ranges from 2 to 12, in discrete values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# frequency of target variable\n",
    "train1.domain1_score.value_counts().sort_index().plot.bar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we simplified and cleaned the data, we turn to modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy intro\n",
    "https://spacy.io/usage/models#languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Here', 'are', 'two', 'sentences', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(u\"Hello, world. Here are two sentences.\")\n",
    "print([t.text for t in doc])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ich', 'bin', 'ein', 'Berliner', '.']\n"
     ]
    }
   ],
   "source": [
    "nlp_de = spacy.load(\"de_core_news_sm\")\n",
    "doc_de = nlp_de(u\"Ich bin ein Berliner.\")\n",
    "print([t.text for t in doc_de])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u\"Peach emoji is where it has always been. Peach is the superior \"\n",
    "          u\"emoji. It's outranking eggplant üçë \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_chunks = list(doc.noun_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(doc.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Peach is the superior emoji.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[1].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x9 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 19 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze = vectorizer.build_analyzer()\n",
    "analyze(\"This is a text document to analyze.\") ==(['this', 'is', 'text', 'document', 'to', 'analyze'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# columns\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 1, 0, 1, 1, 0, 1],\n",
       "       [1, 0, 0, 0, 1, 0, 1, 1, 0],\n",
       "       [0, 1, 1, 1, 0, 0, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each row is a document\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find column index\n",
    "vectorizer.vocabulary_.get('document')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bi', 'grams', 'are', 'cool', 'bi grams', 'grams are', 'are cool']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2-grams of words\n",
    "# \n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2),\n",
    "                                     token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "analyze = bigram_vectorizer.build_analyzer()\n",
    "analyze('Bi-grams are cool!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2 = bigram_vectorizer.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0],\n",
       "       [0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_index = bigram_vectorizer.vocabulary_.get('is this')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_2[:, feature_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words\n",
    "Words like 'the' and 'and'. Usually we remove stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-idf\n",
    "The idf term adds a 1 in the numerator and denominator, compared to the formulas given in Manning's book:\n",
    "\n",
    "$$ idf(t) = \\log \\frac{1+n}{1+df(t)} + 1 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=False, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer(smooth_idf=False)  # 1 added to idf, not to df in denom \n",
    "transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<6x3 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "counts = [[3, 0, 1],\n",
    "          [2, 0, 0],\n",
    "          [3, 0, 0],\n",
    "          [4, 0, 0],\n",
    "          [3, 2, 0],\n",
    "          [3, 0, 2]]\n",
    "\n",
    "tfidf = transformer.fit_transform(counts)\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.81940995, 0.        , 0.57320793],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.47330339, 0.88089948, 0.        ],\n",
       "       [0.58149261, 0.        , 0.81355169]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.85151335, 0.        , 0.52433293],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.55422893, 0.83236428, 0.        ],\n",
       "       [0.63035731, 0.        , 0.77630514]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer = TfidfTransformer()\n",
    "transformer.fit_transform(counts).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 2.25276297, 1.84729786])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# idf of each term\n",
    "transformer.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.43877674, 0.54197657, 0.43877674, 0.        ,\n",
       "        0.        , 0.35872874, 0.        , 0.43877674],\n",
       "       [0.        , 0.40412895, 0.        , 0.40412895, 0.        ,\n",
       "        0.63314609, 0.33040189, 0.        , 0.40412895],\n",
       "       [0.55280532, 0.        , 0.        , 0.        , 0.55280532,\n",
       "        0.        , 0.28847675, 0.55280532, 0.        ],\n",
       "       [0.        , 0.43877674, 0.54197657, 0.43877674, 0.        ,\n",
       "        0.        , 0.35872874, 0.        , 0.43877674]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine CountVectorizer and TfidfTransformer into single model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text features extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html#sphx-glr-download-auto-examples-model-selection-grid-search-text-feature-extraction-py\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "import nltk\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define a pipeline combining a text feature extractor with a simple\n",
    "# classifier\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier(tol=1e-3)),\n",
    "])\n",
    "\n",
    "# data\n",
    "df = pd.read_excel('asap-aes/training_set_rel3.xlsx')\n",
    "data = list(df[df.essay_set==1].essay)\n",
    "target = df.domain1_score.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['num_chars', 'num_words', 'num_commas', 'num_apostrophies', 'sent_end_punc', 'avg_sent_len', 'avg_word_len']\n"
     ]
    }
   ],
   "source": [
    "class Ease_feature_extractor:\n",
    "    def __init__(self):\n",
    "        self.__feature_names = ['num_chars', 'num_words', 'num_commas', 'num_apostrophies', 'sent_end_punc', 'avg_sent_len', 'avg_word_len']\n",
    "        self.__features = defaultdict()\n",
    "        for name in self.__feature_names:\n",
    "            self.__features[name] = []\n",
    "        \n",
    "    def fit_transform(self, raw_documents):\n",
    "        assert type(raw_documents)==list\n",
    "        for doc in raw_documents:\n",
    "            \n",
    "            # Length features\n",
    "            self.__features['num_chars'].append(len(re.sub(r\"\\s+\", \"\", doc)))\n",
    "            self.__features['num_words'].append(len(nltk.RegexpTokenizer(r'\\w+').tokenize(doc)))\n",
    "            self.__features['num_commas'].append(len(nltk.RegexpTokenizer(r',').tokenize(doc)))\n",
    "            self.__features['num_apostrophies'].append(len(nltk.RegexpTokenizer(r\"'\").tokenize(doc)))\n",
    "            self.__features['sent_end_punc'].append(len(nltk.RegexpTokenizer(r'[.?!]').tokenize(doc)))\n",
    "            \n",
    "            sentence_lengths = [len(nltk.RegexpTokenizer(r'\\w+').tokenize(sentence)) for sentence in nltk.sent_tokenize(doc)]\n",
    "            self.__features['avg_sent_len'].append(sum(sentence_lengths)/len(sentence_lengths))\n",
    "            \n",
    "            words = [len(word) for word in nltk.RegexpTokenizer(r'\\w+').tokenize(doc)]\n",
    "            self.__features['avg_word_len'].append(sum(words)/len(words))\n",
    "            \n",
    "            # tf-idf\n",
    "            vectorizer = TfidfVectorizer(sublinear_tf=True, use_idf=True)\n",
    "            \n",
    "            vectorizer = TfidfVectorizer(max_df=0.5, \n",
    "                                         max_features=opts.n_features,\n",
    "                                         stop_words='english',\n",
    "                                         use_idf=opts.use_idf)\n",
    "            \n",
    "            X = vectorizer.fit_transform(data)\n",
    "            \n",
    "            svd = TruncatedSVD(opts.n_components)\n",
    "            #normalizer = Normalizer(copy=False)\n",
    "            lsa = make_pipeline(svd, normalizer)\n",
    "\n",
    "            X = lsa.fit_transform(X)\n",
    "            \n",
    "            return self.__features\n",
    "            \n",
    "            \n",
    "    def get_feature_names(self):\n",
    "        return self.__feature_names\n",
    "\n",
    "    \n",
    "fe_ex = Ease_feature_extractor()\n",
    "X = fe_ex.fit_transform(data)\n",
    "print(fe_ex.get_feature_names())        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fe_ex.features['num_chars']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = ['a','b','c']\n",
    "d=defaultdict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['a'])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['a']=1\n",
    "d.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(data).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1783, 15735)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'tfidf', 'clf']\n",
      "parameters:\n",
      "{'clf__alpha': (1e-05, 1e-06),\n",
      " 'clf__max_iter': (20,),\n",
      " 'clf__penalty': ('l2', 'elasticnet'),\n",
      " 'vect__max_df': (0.5, 0.75, 1.0),\n",
      " 'vect__ngram_range': ((1, 1), (1, 2))}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1783, 12978]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-778a58b56ffb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"done in %0.3fs\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    627\u001b[0m             \u001b[0mrefit_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'score'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m         \u001b[0mn_splits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_n_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 205\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1783, 12978]"
     ]
    }
   ],
   "source": [
    "# modeling\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    # 'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    # 'tfidf__use_idf': (True, False),\n",
    "    # 'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__max_iter': (20,),\n",
    "    'clf__alpha': (0.00001, 0.000001),\n",
    "    'clf__penalty': ('l2', 'elasticnet'),\n",
    "    # 'clf__max_iter': (10, 50, 80),\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(Pipeline,\n",
    "                           parameters,\n",
    "                           cv=5,\n",
    "                           n_jobs=-1,\n",
    "                           verbose=1)\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(parameters)\n",
    "t0 = time()\n",
    "grid_search.fit(data, target)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
